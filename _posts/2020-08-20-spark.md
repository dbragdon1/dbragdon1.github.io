---
layout: post
title: Amazon Cellphone Review Sentiment with PySpark
---

Here I'm going to be using PySpark to create a sentiment model for amazon review data using Logistic Regression. The dataset was collected from [Professor Julian McAuley's Amazon product dataset](https://jmcauley.ucsd.edu/data/amazon/). I will be using a subset of the data titled "Cell Phones and Accessories", just to keep to a specific sentiment domain. 

First, we need to import some necessary libraries for constructing the model, preprocessing the data, and evaluating the results.

```python

from pyspark.sql import SparkSession, SQLContext
```

And now we can build our spark session:

```python
spark = SparkSession.builder.master("local[*]") \
                            .config('spark.driver.memory', '15g') \
                            .appName('amazon_phones').getOrCreate()
sc = spark.sparkContext
```

Now we need to load the data in. We can load it directly from the link given above with the following function:

```python
import json
import urllib.request
import gzip


def load_amazon_cellphones(num_examples):
    link = "http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz"
    stream = urllib.request.urlopen(link)
    file = gzip.open(stream)
    lines = []
    if num_examples == -1:
        for i, line in enumerate(file):
            lines.append(json.loads(line))
    else:
        for i, line in enumerate(file):
            lines.append(json.loads(line))
            if i == num_examples - 1:
                break
    return lines
```

This function locates and unzips the date directly from the source, and allows for you to paritally load the whole dataset in. For example, if you only wanted to load in 100 examples, you could run

```python
dataset = load_amazon_cellphones(100)
```
I found this to be really helpful when troubleshooting and testing the code. For this article, we're going to use every row, meaning we have to run the following line:

```python
dataset = load_amazon_cellphones(-1)
```

The data is currently labeled on a 1-to-5 rating scale. To simplify things, I mapped these labels to a binary classification problem with the following function:

```python
def create_categories(x):
  if x >= 4.0:
    return 1.0
  else:
    return 0.0
```

The problem has now become "can we predict if a user really liked a product?" Anything less than a score of 4.0 means that a user is either just O.K. with, or really dislikes, a product. 

Now we need to create a method for preprocessing our data:

```python
import re
from nltk.corpus import stopwords
from string import punctuation
import nltk

nltk.download('stopwords')
sw = list(set(stopwords.words('english')))

def clean(review):
  #removing numbers
  review = re.sub('[0-9]', '', review).lower()
  #removing punctuation
  review = review.translate(str.maketrans('', '', punctuation))
  return review
```

Now let's only get the useful columns from our data, which are `reviewText` and `overall`.

```python
relevant_info = [(line['reviewText'], 
                 line['overall']) 
                 for line in dataset]
```

And finally, we can get to parallelizing and preprocessing:

```python
#convert data to rdd
rdd = sc.parallelize(relevant_info)
#remove neutral reviews
rdd = rdd.filter(lambda x: x[1] != 3.0)
#map categories to binary labels
rdd = rdd.map(lambda x: (x[0], create_categories(x[1])))
#clean reviews
rdd = rdd.map(lambda x: (clean(x[0]), x[1]))
#remove empty rows
rdd = rdd.filter(lambda x: x[0] != '')
```
Now we can convert the RDD to a DataFrame object:

```python
schema = ['review', 'label']
data = rdd.toDF(schema = schema)
data.show()
```

You should get a result that looks like this:

![firsttable]('images/table1.png')


```python
from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from pyspark.mllib.evaluation import BinaryClassificationMetrics
from pyspark.sql.functions import col
```


